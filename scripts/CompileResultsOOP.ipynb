{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resultSet:\n",
    "    def __init__(self, results_dir, prop_names, prop_targets, init_data_path, head=None, tolerance=.001, only_check_SMILES=True, n_core=18, \n",
    "                 ):\n",
    "        self.parent_dir = results_dir\n",
    "        os.chdir(self.parent_dir)\n",
    "        self.prop_targets = prop_targets\n",
    "        self.n_prop = len(self.prop_targets)\n",
    "        self.prop_names = prop_names\n",
    "        self.all_files = [f for f in listdir(self.parent_dir) if isfile(join(self.parent_dir, f))]\n",
    "        self.results_files = [f for f in self.all_files if re.match('^results.[0-9]+$', f) != None]\n",
    "        self.only_check_SMILES = only_check_SMILES\n",
    "        self.tolerance = tolerance\n",
    "        self.fp_df_path = 'fp_df.csv'\n",
    "        self.init_data_path = init_data_path\n",
    "        self.n_core = n_core\n",
    "        self.head = head\n",
    "        \n",
    "    def resultsToList(self, path_to_results):\n",
    "        '''\n",
    "        Load results from path_to_results as list of unique tuples\n",
    "        '''\n",
    "        start_append = False\n",
    "        data = set()\n",
    "        float_str = ','.join(['float(prop%s)' %i for i in range(self.n_prop)])\n",
    "        parse_str = ','.join(['prop%s' %i for i in range(self.n_prop)])\n",
    "        conditions = ' and '.join(['prop%s%s'%( i,self.prop_targets[i] ) for i in range(self.n_prop)])\n",
    "        with open(path_to_results, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'Done' in line:\n",
    "                    start_append = True\n",
    "                elif start_append:\n",
    "                    exec( '_,_,target,_,%s = %s' %( parse_str, line.split() ) )\n",
    "                    exec( '%s = %s' %( parse_str,float_str ) ) \n",
    "                    exec( 'if %s: data.add((target,%s))' %( conditions,parse_str ) )\n",
    "        return data\n",
    "    def create_fp_input(self):\n",
    "        '''\n",
    "        Create input for 'fp' command\n",
    "        '''\n",
    "        f = ('file_dataset = ./smiles_df.csv\\n'\n",
    "        'col_smiles = SMILES\\n'\n",
    "        'col_X = aT bT m e\\n'\n",
    "        'col_id = ID\\n'\n",
    "        'file_fingerprint = fp_df.csv\\n'\n",
    "        'polymer_fp_version = 2\\n'\n",
    "        'ismolecule = 0\\n'\n",
    "        'drop_failed_rows = 0\\n'\n",
    "        'ncore = %s\\n' %self.n_core)\n",
    "        text_file = open('fp_input', \"w\")\n",
    "        text_file.write(f)\n",
    "        text_file.close()\n",
    "        \n",
    "    def _run_fp(self):\n",
    "        '''\n",
    "        Run fp command. Should only be called after create_fp_input\n",
    "        '''\n",
    "        os.system('fp fp_input') \n",
    "    \n",
    "    def collectUniqueResults(self):\n",
    "        '''\n",
    "        Return all unique-SMILES polymers with superior property values\n",
    "        '''\n",
    "        inter_epoch_data = set()\n",
    "        for f in self.results_files:\n",
    "            inter_epoch_data = inter_epoch_data.union(self.resultsToList(f))\n",
    "        #self.inter_epoch_data = inter_epoch_data\n",
    "        return inter_epoch_data\n",
    "    \n",
    "    def prep_pre_fp_df(self):\n",
    "        '''\n",
    "        Prepare dataframe for fingerprinting\n",
    "        '''\n",
    "        inter_epoch_data = self.collectUniqueResults()\n",
    "        pre_fp_df = pd.DataFrame(inter_epoch_data, columns=['SMILES']+self.prop_names) #create df to use for fingerprinting\n",
    "        if self.head != None:\n",
    "            pre_fp_df = pre_fp_df.head(self.head)\n",
    "        pre_fp_df['ID'] = ['ID_%s' %ind for ind in pre_fp_df.index] #create ID for polymers\n",
    "        if self.only_check_SMILES == True:\n",
    "            pre_fp_df.drop_duplicates(subset=['SMILES'], inplace=True) #drop duplicate SMILES\n",
    "        self.pre_fp_df = pre_fp_df\n",
    "        self.pre_fp_df.to_csv('smiles_df.csv')\n",
    "    \n",
    "    def runFP(self):\n",
    "        '''\n",
    "        Run fingerprinting\n",
    "        '''\n",
    "        self.prep_pre_fp_df()\n",
    "        self.create_fp_input() \n",
    "        print(\"Starting to Fingerprint Valuable Polymers from Results Set\")\n",
    "        self._run_fp() #run fingerprinting\n",
    "        print(\"Finished Fingerprinting Valuable Polymers from Results Set\")\n",
    "        \n",
    "    \n",
    "    def dropWithTolerance(self, df, cols_to_consider):\n",
    "        '''\n",
    "        Drop all duplicates from data frame\n",
    "        '''\n",
    "        #drop duplicates with tolerance\n",
    "        np_df = df[cols_to_consider].to_numpy()\n",
    "        keep_rows = []\n",
    "        duplicate_rows = []\n",
    "        tol = [self.tolerance]*np_df.shape[1]\n",
    "        for ind, row in enumerate(np_df):\n",
    "            if ind not in duplicate_rows:\n",
    "                keep_rows += [ind]\n",
    "                diffs = np.abs(np.asarray(row[None, :]) - np_df)\n",
    "                matching_inds = np.nonzero((diffs <= tol).all(1))[0].tolist()\n",
    "                try:\n",
    "                    matching_inds.remove(ind)\n",
    "                except:\n",
    "                    pass\n",
    "                duplicate_rows += matching_inds\n",
    "        df = df.iloc[keep_rows, :]\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def dropWithToleranceFromReference(self, df, ref):\n",
    "        np_df = df[list(self.fp_intersect)].to_numpy()\n",
    "\n",
    "        tol = [self.tolerance]*len(self.fp_intersect)\n",
    "\n",
    "        np_reduced = ref[list(self.fp_intersect)].to_numpy()\n",
    "        smiles_df = pd.read_csv('smiles_df.csv')\n",
    "        new_polymers = []\n",
    "        keep_inds = []\n",
    "        for i in range(self.n_prop):\n",
    "            exec('pvs_%s = []' %i)\n",
    "        fp_vals = []\n",
    "        for ind, row in enumerate(np_df):\n",
    "            smiles = smiles_df.iloc[ind]['SMILES']\n",
    "            for i in range(self.n_prop):\n",
    "                #pv = df.iloc[ind]['Property Value']\n",
    "                exec(\"pv_%s = smiles_df.iloc[ind]['%s']\" %( i, self.prop_names[i]))\n",
    "            diffs = np.abs(np.asarray(row[None, :]) - np_reduced)\n",
    "            matching_inds = np.nonzero((diffs <= tol).all(1))[0].tolist()\n",
    "            if len(matching_inds) == 0:\n",
    "                new_polymers.append(smiles)\n",
    "                keep_inds.append(ind)\n",
    "                for i in range(self.n_prop):\n",
    "                    #pvs.append(pv)\n",
    "                    exec(\"pvs_%s.append(pv_%s)\" %( i,i ))\n",
    "                fp_vals.append(row)\n",
    "        print(\"%s Valid, Novel, Unique (in fingerprint space) Polymers have been generated\" %len(new_polymers))\n",
    "        #new_polymers_df = pd.DataFrame({'SMILES': new_polymers, \"Band gap\": pvs})\n",
    "        new_polymers_df = pd.DataFrame(data=fp_vals, columns=self.fp_intersect)\n",
    "        new_polymers_df['SMILES'] = new_polymers\n",
    "        for i in range(self.n_prop):\n",
    "            #new_polymers_df['Band gap'] = pvs\n",
    "            exec(\"new_polymers_df['%s'] = pvs_%s\" %( self.prop_names[i],i ))\n",
    "        return new_polymers_df\n",
    "\n",
    "        \n",
    "    def vuPolymers(self):\n",
    "        '''\n",
    "        Return all valuable, unique polymers in resultSet\n",
    "        '''\n",
    "        self.runFP()\n",
    "        df = pd.read_csv('fp_df.csv') #load df containing fingerprint for each SMILES\n",
    "        df = df.iloc[df.dropna().index] #drop NA\n",
    "        self.fp_cols = [col for col in df.keys() if col != 'ID' and 'Unnamed' not in col] #get columns which contain fingerprint\n",
    "        df = self.dropWithTolerance(df, self.fp_cols)\n",
    "        print(\"%s Valid, Unique (in fingerprint space) Polymers have been generated\" %len(df))\n",
    "        return df\n",
    "    \n",
    "    def vnuPolymers(self):\n",
    "        '''\n",
    "        Return all valuable, novel, unique polymers in resultSet\n",
    "        '''\n",
    "        df = self.vuPolymers()\n",
    "        df = df.merge(self.pre_fp_df, on='ID')\n",
    "        initial_df = pd.read_csv(self.init_data_path) #load initial dataset\n",
    "        initial_fp_cols = [col for col in initial_df.keys() if col != 'id' and 'Unnamed' not in col and 'bandgap' not in col]\n",
    "\n",
    "        self.fp_intersect = set(self.fp_cols).intersection(initial_fp_cols)\n",
    "\n",
    "        reduced_df = initial_df[list(self.fp_intersect) + ['smiles']]\n",
    "        \n",
    "        new_polymers_df = self.dropWithToleranceFromReference(df, reduced_df)\n",
    "        return new_polymers_df\n",
    "    \n",
    "    def save_vnu_polymers(self):\n",
    "        new_polymers_df = self.vnuPolymers()\n",
    "        self.now = datetime.datetime.now().strftime(\"%I_%M%p_on_%B_%d_%Y\")\n",
    "        save_file = 'new_polymers_%s.csv' %self.now\n",
    "        new_polymers_df.to_csv(save_file)\n",
    "        \n",
    "        print(\"New Polymers saved to %s\" %save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = resultSet('/home/rgur/g2g/eg_and_tg/lr_0.001_bs_32_depthT_6_depthG_8_hs_300/results/', ['Bandgap', 'Tg'], ['>=6', '>=500'], '/home/rgur/g2g/eg_and_tg/raw_data/fp_df.csv', tolerance=.001, n_core=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to Fingerprint Valuable Polymers from Results Set\n",
      "Finished Fingerprinting Valuable Polymers from Results Set\n",
      "586 Valid, Unique (in fingerprint space) Polymers have been generated\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['smiles'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a675e3f7800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vnu_polymers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1888287d03c6>\u001b[0m in \u001b[0;36msave_vnu_polymers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_vnu_polymers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mnew_polymers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvnuPolymers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%I_%M%p_on_%B_%d_%Y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0msave_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'new_polymers_%s.csv'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1888287d03c6>\u001b[0m in \u001b[0;36mvnuPolymers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp_intersect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_fp_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mreduced_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp_intersect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'smiles'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mnew_polymers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropWithToleranceFromReference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rgur/.conda/envs/cse6250/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rgur/.conda/envs/cse6250/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rgur/.conda/envs/cse6250/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rgur/.conda/envs/cse6250/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'loc'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not in index\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['smiles'] not in index\""
     ]
    }
   ],
   "source": [
    "rs.save_vnu_polymers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cse6250)",
   "language": "python",
   "name": "cse6250"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
