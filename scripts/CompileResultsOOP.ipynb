{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rishi_utils as ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resultSet:\n",
    "    def __init__(self, results_dir, prop_names, prop_targets, init_data_path, head=None, tolerance=.001, only_check_SMILES=True, n_core=18, \n",
    "                 ):\n",
    "        self.parent_dir = results_dir\n",
    "        os.chdir(self.parent_dir)\n",
    "        self.prop_targets = prop_targets\n",
    "        self.n_prop = len(self.prop_targets)\n",
    "        self.prop_names = prop_names\n",
    "        self.all_files = [f for f in listdir(self.parent_dir) if isfile(join(self.parent_dir, f))]\n",
    "        self.results_files = [f for f in self.all_files if re.match('^results.[0-9]+$', f) != None]\n",
    "        self.only_check_SMILES = only_check_SMILES\n",
    "        self.tolerance = tolerance\n",
    "        self.fp_df_path = 'fp_df.csv'\n",
    "        self.init_data_path = init_data_path\n",
    "        self.n_core = n_core\n",
    "        self.head = head\n",
    "        \n",
    "    def resultsToList(self, path_to_results):\n",
    "        '''\n",
    "        Load results from path_to_results as list of unique tuples\n",
    "        '''\n",
    "        start_append = False\n",
    "        data = set()\n",
    "        float_str = ','.join(['float(prop%s)' %i for i in range(self.n_prop)])\n",
    "        parse_str = ','.join(['prop%s' %i for i in range(self.n_prop)])\n",
    "        conditions = ' and '.join(['prop%s%s'%( i,self.prop_targets[i] ) for i in range(self.n_prop)])\n",
    "        with open(path_to_results, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'Done' in line:\n",
    "                    start_append = True\n",
    "                elif start_append:\n",
    "                    exec( '_,_,target,_,%s = %s' %( parse_str, line.split() ) )\n",
    "                    exec( '%s = %s' %( parse_str,float_str ) ) \n",
    "                    exec( 'if %s: data.add((target,%s))' %( conditions,parse_str ) )\n",
    "        return data\n",
    "    def create_fp_input(self):\n",
    "        '''\n",
    "        Create input for 'fp' command\n",
    "        '''\n",
    "        f = ('file_dataset = ./smiles_df.csv\\n'\n",
    "        'col_smiles = SMILES\\n'\n",
    "        'col_X = aT bT m e\\n'\n",
    "        'col_id = ID\\n'\n",
    "        'file_fingerprint = fp_df.csv\\n'\n",
    "        'polymer_fp_version = 2\\n'\n",
    "        'ismolecule = 0\\n'\n",
    "        'drop_failed_rows = 0\\n'\n",
    "        'ncore = %s\\n' %self.n_core)\n",
    "        text_file = open('fp_input', \"w\")\n",
    "        text_file.write(f)\n",
    "        text_file.close()\n",
    "        \n",
    "    def _run_fp(self):\n",
    "        '''\n",
    "        Run fp command. Should only be called after create_fp_input\n",
    "        '''\n",
    "        os.system('fp fp_input') \n",
    "    \n",
    "    def collectUniqueResults(self):\n",
    "        '''\n",
    "        Return all unique-SMILES polymers with superior property values\n",
    "        '''\n",
    "        inter_epoch_data = set()\n",
    "        for f in self.results_files:\n",
    "            inter_epoch_data = inter_epoch_data.union(self.resultsToList(f))\n",
    "        #self.inter_epoch_data = inter_epoch_data\n",
    "        return inter_epoch_data\n",
    "    \n",
    "    def prep_pre_fp_df(self):\n",
    "        '''\n",
    "        Prepare dataframe for fingerprinting\n",
    "        '''\n",
    "        inter_epoch_data = self.collectUniqueResults()\n",
    "        pre_fp_df = pd.DataFrame(inter_epoch_data, columns=['SMILES']+self.prop_names) #create df to use for fingerprinting\n",
    "        if self.head != None:\n",
    "            pre_fp_df = pre_fp_df.head(self.head)\n",
    "        pre_fp_df['ID'] = ['ID_%s' %ind for ind in pre_fp_df.index] #create ID for polymers\n",
    "        if self.only_check_SMILES == True:\n",
    "            pre_fp_df.drop_duplicates(subset=['SMILES'], inplace=True) #drop duplicate SMILES\n",
    "        self.pre_fp_df = pre_fp_df\n",
    "        self.pre_fp_df.to_csv('smiles_df.csv')\n",
    "        self.n_valuable = len(self.pre_fp_df)\n",
    "    \n",
    "    def runFP(self):\n",
    "        '''\n",
    "        Run fingerprinting\n",
    "        '''\n",
    "        self.prep_pre_fp_df()\n",
    "        self.create_fp_input() \n",
    "        print(\"Starting to Fingerprint %s Valuable, Unique (in SMILES space) Polymers from Results Set\" %self.n_valuable)\n",
    "        self._run_fp() #run fingerprinting\n",
    "        print(\"Finished Fingerprinting Valuable, Unique (in SMILES space) Polymers from Results Set\")\n",
    "        \n",
    "    \n",
    "    def dropWithTolerance(self, df, cols_to_consider):\n",
    "        '''\n",
    "        Drop all duplicates from data frame\n",
    "        '''\n",
    "        #drop duplicates with tolerance\n",
    "        np_df = df[cols_to_consider].to_numpy()\n",
    "        keep_rows = []\n",
    "        duplicate_rows = []\n",
    "        tol = [self.tolerance]*np_df.shape[1]\n",
    "        for ind, row in enumerate(np_df):\n",
    "            if ind not in duplicate_rows:\n",
    "                keep_rows += [ind]\n",
    "                diffs = np.abs(np.asarray(row[None, :]) - np_df)\n",
    "                matching_inds = np.nonzero((diffs <= tol).all(1))[0].tolist()\n",
    "                try:\n",
    "                    matching_inds.remove(ind)\n",
    "                except:\n",
    "                    pass\n",
    "                duplicate_rows += matching_inds\n",
    "        df = df.iloc[keep_rows, :]\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def dropWithToleranceFromReference(self, df, ref):\n",
    "        np_df = df[list(self.fp_intersect)].to_numpy()\n",
    "\n",
    "        tol = [self.tolerance]*len(self.fp_intersect)\n",
    "\n",
    "        np_reduced = ref[list(self.fp_intersect)].to_numpy()\n",
    "        smiles_df = pd.read_csv('smiles_df.csv')\n",
    "        new_polymers = []\n",
    "        keep_inds = []\n",
    "        for i in range(self.n_prop):\n",
    "            exec('pvs_%s = []' %i)\n",
    "        fp_vals = []\n",
    "        for ind, row in enumerate(np_df):\n",
    "            smiles = smiles_df.iloc[ind]['SMILES']\n",
    "            for i in range(self.n_prop):\n",
    "                #pv = df.iloc[ind]['Property Value']\n",
    "                exec(\"pv_%s = smiles_df.iloc[ind]['%s']\" %( i, self.prop_names[i]))\n",
    "            diffs = np.abs(np.asarray(row[None, :]) - np_reduced)\n",
    "            matching_inds = np.nonzero((diffs <= tol).all(1))[0].tolist()\n",
    "            if len(matching_inds) == 0:\n",
    "                new_polymers.append(smiles)\n",
    "                keep_inds.append(ind)\n",
    "                for i in range(self.n_prop):\n",
    "                    #pvs.append(pv)\n",
    "                    exec(\"pvs_%s.append(pv_%s)\" %( i,i ))\n",
    "                fp_vals.append(row)\n",
    "        print(\"%s Valuable, Novel, Unique (in fingerprint space) Polymers have been generated\" %len(new_polymers))\n",
    "        #new_polymers_df = pd.DataFrame({'SMILES': new_polymers, \"Band gap\": pvs})\n",
    "        new_polymers_df = pd.DataFrame(data=fp_vals, columns=self.fp_intersect)\n",
    "        new_polymers_df['SMILES'] = new_polymers\n",
    "        for i in range(self.n_prop):\n",
    "            #new_polymers_df['Band gap'] = pvs\n",
    "            exec(\"new_polymers_df['%s'] = pvs_%s\" %( self.prop_names[i],i ))\n",
    "        return new_polymers_df\n",
    "\n",
    "        \n",
    "    def vuPolymers(self):\n",
    "        '''\n",
    "        Return all valuable, unique polymers in resultSet\n",
    "        '''\n",
    "        self.runFP()\n",
    "        df = pd.read_csv('fp_df.csv') #load df containing fingerprint for each SMILES\n",
    "        df = df.iloc[df.dropna().index] #drop NA\n",
    "        self.fp_cols = [col for col in df.keys() if col != 'ID' and 'Unnamed' not in col] #get columns which contain fingerprint\n",
    "        df = self.dropWithTolerance(df, self.fp_cols)\n",
    "        print(\"%s Valuable, Unique (in fingerprint space) Polymers have been generated\" %len(df))\n",
    "        return df\n",
    "    \n",
    "    def vnuPolymers(self):\n",
    "        '''\n",
    "        Return all valuable, novel, unique polymers in resultSet\n",
    "        '''\n",
    "        df = self.vuPolymers()\n",
    "        df = df.merge(self.pre_fp_df, on='ID')\n",
    "        initial_df = pd.read_csv(self.init_data_path) #load initial dataset\n",
    "        initial_fp_cols = [col for col in initial_df.keys() if col != 'id' and 'Unnamed' not in col and 'bandgap' not in col]\n",
    "\n",
    "        self.fp_intersect = set(self.fp_cols).intersection(initial_fp_cols)\n",
    "\n",
    "        reduced_df = initial_df[list(self.fp_intersect) + ['smiles']]\n",
    "        \n",
    "        new_polymers_df = self.dropWithToleranceFromReference(df, reduced_df)\n",
    "        return new_polymers_df\n",
    "    \n",
    "    def save_vnu_polymers(self):\n",
    "        new_polymers_df = self.vnuPolymers()\n",
    "        new_polymers_df['Uniqueness'] = ru.struct_uniqueness(new_polymers_df['SMILES'].tolist())\n",
    "        new_polymers_df = new_polymers_df.sort_values(by='Uniqueness', ascending=False)\n",
    "        self.now = datetime.datetime.now().strftime(\"%I_%M%p_on_%B_%d_%Y\")\n",
    "        save_file = 'new_polymers_%s.csv' %self.now\n",
    "        new_polymers_df.to_csv(save_file)\n",
    "        \n",
    "        print(\"New Polymers saved to %s\" %save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = resultSet('/data/rgur/g2g/eg_and_tg/relaxed/lr_0.001_bs_32_depthT_6_depthG_8_hs_300/results/', \n",
    "               ['Bandgap', 'Tg'], ['>=6', '>=500'], '/data/rgur/g2g/eg_and_tg/raw_data/fp_df.csv', tolerance=.001, n_core=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to Fingerprint 342 Valuable, Unique (in SMILES space) Polymers from Results Set\n",
      "Finished Fingerprinting Valuable, Unique (in SMILES space) Polymers from Results Set\n",
      "267 Valuable, Unique (in fingerprint space) Polymers have been generated\n",
      "265 Valuable, Novel, Unique (in fingerprint space) Polymers have been generated\n",
      "New Polymers saved to new_polymers_07_19AM_on_May_07_2020.csv\n"
     ]
    }
   ],
   "source": [
    "rs.save_vnu_polymers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(cse6250)",
   "language": "python",
   "name": "cse6250"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
