{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resultSet:\n",
    "    def __init__(self, results_dir, prop_delta, head=None, tolerance=.001, only_check_SMILES=True, n_core=18, \n",
    "                 init_data_path='~/CS6250_project/raw_data/fp_all_data.csv'):\n",
    "        self.parent_dir = results_dir\n",
    "        os.chdir(self.parent_dir)\n",
    "        self.prop_delta = prop_delta\n",
    "        self.all_files = [f for f in listdir(self.parent_dir) if isfile(join(self.parent_dir, f))]\n",
    "        self.results_files = [f for f in self.all_files if re.match('^results.[0-9]+$', f) != None]\n",
    "        self.only_check_SMILES = only_check_SMILES\n",
    "        self.tolerance = tolerance\n",
    "        self.fp_df_path = 'fp_df.csv'\n",
    "        self.init_data_path = init_data_path\n",
    "        self.n_core = n_core\n",
    "        self.head = head\n",
    "        \n",
    "    def resultsToList(self, path_to_results):\n",
    "        '''\n",
    "        Load results from path_to_results as list of unique tuples\n",
    "        '''\n",
    "        start_append = False\n",
    "        data = set()\n",
    "        with open(path_to_results, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'Done' in line:\n",
    "                    start_append = True\n",
    "                elif start_append:\n",
    "                    #print line.split()\n",
    "                    _, _, target, _, bg = line.split()\n",
    "                    bg = float(bg)\n",
    "                    if bg > self.prop_delta:\n",
    "                        data.add((target, bg))\n",
    "        return data\n",
    "    \n",
    "    def create_fp_input(self):\n",
    "        '''\n",
    "        Create input for 'fp' command\n",
    "        '''\n",
    "        f = ('file_dataset = ./smiles_df.csv\\n'\n",
    "        'col_smiles = SMILES\\n'\n",
    "        'col_X = aT bT m e\\n'\n",
    "        'col_id = ID\\n'\n",
    "        'file_fingerprint = fp_df.csv\\n'\n",
    "        'polymer_fp_version = 2\\n'\n",
    "        'ismolecule = 0\\n'\n",
    "        'drop_failed_rows = 0\\n'\n",
    "        'ncore = %s\\n' %self.n_core)\n",
    "        text_file = open('fp_input', \"w\")\n",
    "        text_file.write(f)\n",
    "        text_file.close()\n",
    "        \n",
    "    def _run_fp(self):\n",
    "        '''\n",
    "        Run fp command. Should only be called after create_fp_input\n",
    "        '''\n",
    "        os.system('fp fp_input') \n",
    "    \n",
    "    def collectUniqueResults(self):\n",
    "        '''\n",
    "        Return all unique-SMILES polymers with superior property values\n",
    "        '''\n",
    "        inter_epoch_data = set()\n",
    "        for f in self.results_files:\n",
    "            inter_epoch_data = inter_epoch_data.union(self.resultsToList(f))\n",
    "        #self.inter_epoch_data = inter_epoch_data\n",
    "        return inter_epoch_data\n",
    "    \n",
    "    def prep_pre_fp_df(self):\n",
    "        '''\n",
    "        Prepare dataframe for fingerprinting\n",
    "        '''\n",
    "        inter_epoch_data = self.collectUniqueResults()\n",
    "        pre_fp_df = pd.DataFrame(inter_epoch_data, columns=['SMILES', 'Property Value']) #create df to use for fingerprinting\n",
    "        if self.head != None:\n",
    "            pre_fp_df = pre_fp_df.head(self.head)\n",
    "        pre_fp_df['ID'] = ['ID_%s' %ind for ind in pre_fp_df.index] #create ID for polymers\n",
    "        if self.only_check_SMILES == True:\n",
    "            pre_fp_df.drop_duplicates(subset=['SMILES'], inplace=True) #drop duplicate SMILES\n",
    "        self.pre_fp_df = pre_fp_df\n",
    "        self.pre_fp_df.to_csv('smiles_df.csv')\n",
    "    \n",
    "    def runFP(self):\n",
    "        '''\n",
    "        Run fingerprinting\n",
    "        '''\n",
    "        self.prep_pre_fp_df()\n",
    "        self.create_fp_input() \n",
    "        print(\"Starting to Fingerprint Valuable Polymers from Results Set\")\n",
    "        self._run_fp() #run fingerprinting\n",
    "        print(\"Finished Fingerprinting Valuable Polymers from Results Set\")\n",
    "        \n",
    "    \n",
    "    def dropWithTolerance(self, df, cols_to_consider):\n",
    "        '''\n",
    "        Drop all duplicates from data frame\n",
    "        '''\n",
    "        #drop duplicates with tolerance\n",
    "        np_df = df[cols_to_consider].to_numpy()\n",
    "        keep_rows = []\n",
    "        duplicate_rows = []\n",
    "        tol = [self.tolerance]*np_df.shape[1]\n",
    "        for ind, row in enumerate(np_df):\n",
    "            if ind not in duplicate_rows:\n",
    "                keep_rows += [ind]\n",
    "                diffs = np.abs(np.asarray(row[None, :]) - np_df)\n",
    "                matching_inds = np.nonzero((diffs <= tol).all(1))[0].tolist()\n",
    "                try:\n",
    "                    matching_inds.remove(ind)\n",
    "                except:\n",
    "                    pass\n",
    "                duplicate_rows += matching_inds\n",
    "        df = df.iloc[keep_rows, :]\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def dropWithToleranceFromReference(self, df, ref):\n",
    "        np_df = df[list(self.fp_intersect)].to_numpy()\n",
    "\n",
    "        tol = [self.tolerance]*len(self.fp_intersect)\n",
    "\n",
    "        np_reduced = ref[list(self.fp_intersect)].to_numpy()\n",
    "\n",
    "        new_polymers = []\n",
    "        keep_inds = []\n",
    "        pvs = []\n",
    "        for ind, row in enumerate(np_df):\n",
    "            smiles = df.iloc[ind]['SMILES']\n",
    "            pv = df.iloc[ind]['Property Value']\n",
    "            diffs = np.abs(np.asarray(row[None, :]) - np_reduced)\n",
    "            matching_inds = np.nonzero((diffs <= tol).all(1))[0].tolist()\n",
    "            if len(matching_inds) == 0:\n",
    "                new_polymers.append(smiles)\n",
    "                keep_inds.append(ind)\n",
    "                pvs.append(pv)\n",
    "        print(\"%s Valid, Novel, Unique (in fingerprint space) Polymers have been generated\" %len(new_polymers))\n",
    "        new_polymers_df = pd.DataFrame({'SMILES': new_polymers, \"Band gap\": pvs})\n",
    "        return new_polymers_df\n",
    "\n",
    "        \n",
    "    def vuPolymers(self):\n",
    "        '''\n",
    "        Return all valuable, unique polymers in resultSet\n",
    "        '''\n",
    "        self.runFP()\n",
    "        df = pd.read_csv('fp_df.csv') #load df containing fingerprint for each SMILES\n",
    "        df = df.iloc[df.dropna().index] #drop NA\n",
    "        self.fp_cols = [col for col in df.keys() if col != 'ID' and 'Unnamed' not in col] #get columns which contain fingerprint\n",
    "        df = self.dropWithTolerance(df, self.fp_cols)\n",
    "        print(\"%s Valid, Unique (in fingerprint space) Polymers have been generated\" %len(df))\n",
    "        return df\n",
    "    \n",
    "    def vnuPolymers(self):\n",
    "        '''\n",
    "        Return all valuable, novel, unique polymers in resultSet\n",
    "        '''\n",
    "        df = self.vuPolymers()\n",
    "        df = df.merge(self.pre_fp_df, on='ID')\n",
    "        initial_df = pd.read_csv(self.init_data_path) #load initial dataset\n",
    "        initial_fp_cols = [col for col in initial_df.keys() if col != 'id' and 'Unnamed' not in col and 'bandgap' not in col]\n",
    "\n",
    "        self.fp_intersect = set(self.fp_cols).intersection(initial_fp_cols)\n",
    "\n",
    "        reduced_df = initial_df[list(self.fp_intersect) + ['smiles']]\n",
    "        \n",
    "        new_polymers_df = self.dropWithToleranceFromReference(df, reduced_df)\n",
    "        return new_polymers_df\n",
    "    \n",
    "    def save_vnu_polymers(self):\n",
    "        new_polymers_df = self.vnuPolymers()\n",
    "        self.now = datetime.datetime.now().strftime(\"%I_%M%p_on_%B_%d_%Y\")\n",
    "        save_file = 'new_polymers_%s.csv' %self.now\n",
    "        new_polymers_df.to_csv(save_file)\n",
    "        \n",
    "        print(\"New Polymers saved to %s\" %save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = resultSet('/home/rgur/g2g/improved/lr_0.001_bs_32_depthT_6_depthG_8/results/', 6, tolerance=.001, n_core=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to Fingerprint Valuable Polymers from Results Set\n",
      "Finished Fingerprinting Valuable Polymers from Results Set\n",
      "5053 Valid, Unique (in fingerprint space) Polymers have been generated\n",
      "4975 Valid, Novel, Unique (in fingerprint space) Polymers have been generated\n",
      "New Polymers saved to new_polymers_12_35PM_on_April_28_2020.csv\n"
     ]
    }
   ],
   "source": [
    "rs.save_vnu_polymers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [a, a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  1  2  3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=b, columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cse6250)",
   "language": "python",
   "name": "cse6250"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
